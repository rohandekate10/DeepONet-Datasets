{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "import seaborn as sns\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"ticks\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering and forecasting extreme events via active learning in neural operators\n",
    "\n",
    "Citation - Pickering, E., Guth, S., Karniadakis, G. E. & Sapsis, T. P. Discovering and forecasting extreme events via active learning in neural operators. Nat Comput Sci 2, 823–833 (2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) MMT Nonlinear Wave\n",
    "\n",
    "* We seek to map initially observed waves, $u(x, t = 0)$, where $x$ is the spatial variable and $t$ is time, to the QoI: the future spatial maximum $G(x) = ||Re(u(x, t = T; x))||_{\\infty}$, where $T$ is a prescribed prediction horizon.\n",
    "* MMT has complex solutions and therefore the ICs are also complex-valued.\n",
    "* Dispersive nonlinear wave equation used for studying 1D wave turbulence.\n",
    "$$\n",
    "iu_{t} = |\\partial_{x}|^{\\alpha} u + \\lambda |\\partial_{x}|^{-\\beta/4} \\Big(||\\partial_{x}|^{- \\beta /4} u \\Big|^{2} |\\partial_{x}|^{- \\beta /4} u \\Big) + iDu,\n",
    "$$\n",
    "* where $u$ is a complex scalar, \n",
    "* exponents $\\alpha$ and $\\beta$ are chosen model parameters, \n",
    "* and $D$ is a selective Laplacian\n",
    "* This model gives rise to four-wave resonant interactions that, especially when coupled with large scale forcing and small scale damping, produces a family of spectra revealing both direct and inverse cascades.\n",
    "* Transform the equation into wavenumber space.\n",
    "* The pseudodifferential operator $|\\partial_{x}|^{\\alpha}$, via the Fourier transform in space becomes: $\\widehat{|\\partial_{x}|^{\\alpha} u(k)} = |k|^{\\alpha} \\widehat{u} (k)$ where $k$ is the wavenumber in $x$. \n",
    "* This formulation may be similarly defined on a periodic domain. \n",
    "* We choose $\\alpha = 1/2$ and $\\beta = 0$. Above equation reduces to\n",
    "$$\n",
    "\\hat{u}(k)_{t} = -i |k|^{1/2} \\hat{u} (k) - i \\lambda |\\hat{u} (k)|^{2} \\hat{u} (k) + \\widehat{Du}(k) + f(k)\n",
    "$$\n",
    "* where $f(k)$ is a forcing and $\\widehat{Du}(k)$ is a selective Laplacian of the form:\n",
    "$$\n",
    "\\widehat{Du}(k) = \n",
    "\\begin{cases}\n",
    "  -(|k| - k^{*})^2 \\hat{u} k, & |k|>k^{*} \\\\\n",
    "  0, & |k| \\leq k^{*} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "* where $k^{*}$ presents the lower bound of wavenumbers subject to dissipation.\n",
    "* For the model considered in this study we choose $\\lambda = −0.5, k^{*} = 20, f (k) = 0, dt = 0.001$, and a grid that is periodic between $0-1$ with $N_x = 512$ grid points.\n",
    "* To propose a stochastic and complex initial condition, $u(x, t = 0)$, we take the complex-valued kernel\n",
    "$$\n",
    "k(x,x') = \\sigma^{2}_{u} e^{i(x-x')} e^{-\\frac{(x-x')^{2}}{\\ell_{u}}}\n",
    "$$\n",
    "* with $\\sigma^{2}_{u} = 1$ and $\\ell_{u} = 0.35$.\n",
    "* We then parametrize the stochastic initial conditions by a finite number of random variables, $x$, using the Karhunen-Loeve expansion of the kernel’s correlation matrix,\n",
    "$$\n",
    "u(x,t = 0) \\approx \\textbf{x} \\Phi(x), \\space \\forall \\space x \\in [0,1)\n",
    "$$\n",
    "* where $x \\in \\mathbb{C}^m$ is a vector of complex coefficients and both the real and imaginary components of each coefficient are normally distributed with zero mean and diagonal covariance matrix $\\Lambda$.\n",
    "* and $\\{\\Lambda, \\Phi (x)\\}$ contains the first $m$ eigenpairs of the correlation matrix.\n",
    "* This gives the dimension of the parameter space as $2m$ to account for the real and imaginary components of each coefficient. \n",
    "* The presented $2D, 4D, 6D, 20D$ results correspond to $m = 1, 2, 3, 10$. \n",
    "* For all cases, the random variable $x_i$ is restricted to a domain ranging from -6 to 6, in that 6 standard deviations in each direction from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.blackbox import BlackBox\n",
    "from utils.complex_noise import Noise_MMT\n",
    "from utils.gaussian_input import GaussianInputs\n",
    "import scipy.io as sio\n",
    "\n",
    "seed = 1234\n",
    "iter_num = 2\n",
    "n_init = 7\n",
    "init_method = \"lhs\"\n",
    "acq = \"lhs\"\n",
    "lam = 5.0\n",
    "N = 15\n",
    "batch_size = 6\n",
    "model = \"GP\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = 1\n",
    "rank = 3\n",
    "noise = Noise_MMT([0, tf], rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Theta_u to U map for saving localling to be called by Matlab MMT files\n",
    "def Save_U(Theta,nsteps,rank):\n",
    "    y0 = np.zeros((1,nsteps),dtype=np.complex_)\n",
    "    xr = Theta[0:rank]\n",
    "    xi = Theta[rank:(2*rank)]\n",
    "    x = xr + 1j*xi\n",
    "    y0 = noise.get_sample(x)\n",
    "    return y0\n",
    "\n",
    "# Mapping Theta to U with only one input\n",
    "def map_def(theta_rank):\n",
    "    rank = int(theta_rank[1])\n",
    "    theta = theta_rank[0].reshape(rank*2,)\n",
    "    nsteps = 512\n",
    "    y0 = Save_U(theta,nsteps,rank)\n",
    "    return y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if iter_num == 0:\n",
    "        ndim = rank*2\n",
    "        np.random.seed(seed)\n",
    "        noise_var = 0\n",
    "        nsteps = 512\n",
    "        my_map = BlackBox(map_def, noise_var=noise_var)\n",
    "\n",
    "        mean, cov = np.zeros(ndim), np.ones(ndim)\n",
    "        domain = [ [-6, 6] ] * ndim\n",
    "        inputs = GaussianInputs(domain, mean, cov)\n",
    "        Theta = inputs.draw_samples(n_init, init_method)\n",
    "        \n",
    "        save_y0_file = './data/'+'_Seed'+str(seed)+'_ndim'+str(ndim)+'_rank'+str(rank)+'_n_init'+str(n_init)+'_init_method_'+str(init_method)+'_savedata_y0.mat'\n",
    "\n",
    "        y0 = np.zeros((nsteps,n_init),dtype=np.complex_)\n",
    "        for i in range(0,n_init):\n",
    "            y0[:,i] = Save_U(Theta[i,:],nsteps,rank).reshape(nsteps,)\n",
    "            # Save the y0 file to run with matlab\n",
    "        sio.savemat(save_y0_file, {'y0':y0, 'Theta':Theta})        \n",
    "else:\n",
    "    ndim = rank*2\n",
    "    np.random.seed(seed)\n",
    "    noise_var = 0\n",
    "    my_map = BlackBox(map_def, noise_var=noise_var)\n",
    "\n",
    "    mean, cov = np.zeros(ndim), np.ones(ndim)\n",
    "    domain = [ [-6, 6] ] * ndim\n",
    "    inputs = GaussianInputs(domain, mean, cov)\n",
    "    \n",
    "    # Need to determine U\n",
    "    nsteps = 512 # Reset the number of steps for deeponet\n",
    "    x_vals = np.linspace(0, 1, nsteps+1)\n",
    "    x_vals = x_vals[0:-1]\n",
    "        \n",
    "    #Y = my_map.evaluate(Theta,1)    \n",
    "    coarse = 4\n",
    "    # Create the X to U map, which is actually theta to U\n",
    "    def Theta_to_U(Theta,nsteps,coarse,rank):\n",
    "        # We can also coarsen the steps 512 is likely extra fine for Deeponet\n",
    "        Theta = np.atleast_2d(Theta)\n",
    "        U = np.zeros((np.shape(Theta)[0],2*int(nsteps/coarse)),dtype=np.complex_)\n",
    "\n",
    "        # Determine real and imaginary inds\n",
    "        dim = int(np.shape(Theta)[1]/2)\n",
    "        xr = Theta[:,0:(dim)]\n",
    "        xi = Theta[:,dim:dim*2]\n",
    "        x = xr + 1j*xi\n",
    "        Us = np.transpose(noise.get_sample(x))\n",
    "        coarser_inds = np.linspace(0,nsteps-1,int(nsteps/coarse)).astype(int)\n",
    "\n",
    "        real_inds = np.linspace(0,nsteps/coarse*2-2,int(nsteps/coarse)).astype(int)\n",
    "        imag_inds = np.linspace(1,nsteps/coarse*2-1,int(nsteps/coarse)).astype(int)\n",
    "        \n",
    "        U[:,real_inds] = np.real(Us[:,coarser_inds])\n",
    "        U[:,imag_inds] = np.imag(Us[:,coarser_inds])\n",
    "        return U\n",
    "    \n",
    "    def Theta_to_Z(Theta,rank):\n",
    "        Z = np.ones((Theta.shape[0], 1))\n",
    "        return Z\n",
    "\n",
    "    save_path_data_prev = './results/Rank'+str(rank)+'_'+model+'_'+acq+'_Seed'+str(seed)+'_N'+str(N)+'_Batch_'+str(batch_size)+'_Init_'+init_method+'_Iteration'+str(iter_num-1)+'.mat'\n",
    "    load_Y_file = './IC/Rank'+str(rank)+'_'+model+'_Seed'+str(seed)+'_Acq'+acq+'_Iter'+str(iter_num-1)+'_Lam'+str(lam)+'_BatchSize'+str(batch_size)+'_N'+str(N)+'_savedata_Y.mat' \n",
    "    save_y0_file_prev = './IC/Rank'+str(rank)+'_'+model+'_Seed'+str(seed)+'_Acq'+acq+'_Iter'+str(iter_num-1)+'_Lam'+str(lam)+'_BatchSize'+str(batch_size)+'_N'+str(N)+'_savedata_y0.mat'       \n",
    "\n",
    "    if iter_num == 1:\n",
    "            d = sio.loadmat(save_y0_file_prev)\n",
    "            Theta = d['Theta']\n",
    "            d = sio.loadmat(load_Y_file)\n",
    "            Y = d['Y']\n",
    "            print(np.shape(Y))\n",
    "            \n",
    "    else: \n",
    "        d = sio.loadmat(save_path_data_prev)\n",
    "        Theta = d['Theta']\n",
    "        Y = d['Y']\n",
    "    \n",
    "        # Load in the files\n",
    "        d = sio.loadmat(save_y0_file_prev)\n",
    "        theta_opt = d['theta_opt']\n",
    "        d = sio.loadmat(load_Y_file)\n",
    "        Y_opt = d['Y']\n",
    "    \n",
    "        Theta = np.vstack((Theta, theta_opt))\n",
    "        Y = np.vstack((Y, Y_opt))\n",
    "        print(np.shape(Theta))\n",
    "        print(np.shape(Y))\n",
    "    \n",
    "    np.random.seed(np.size(Y))\n",
    "\n",
    "    # These functions are defined for normalizing, standardizing, or flatenining interal to DeepONet\n",
    "    def DNO_Y_transform(x):\n",
    "        x_transform = x\n",
    "        return x_transform\n",
    "\n",
    "    def DNO_Y_itransform(x_transform):\n",
    "        x = x_transform\n",
    "        return x\n",
    "    \n",
    "    # Train the model\n",
    "    np.random.seed(np.size(Y)) # Randomize the seed based on Y size for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to U and G values       \n",
    "u_train = Theta_to_U(Theta,nsteps,coarse,rank)\n",
    "print(f\"Shape of u_train:{u_train.shape}\")\n",
    "y_train = Theta_to_Z(Theta,rank)\n",
    "print(f\"Shape of y_train:{y_train.shape}\")\n",
    "G_train = DNO_Y_transform(Y)\n",
    "print(f\"Shape of G_train:{G_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file = sio.loadmat(\"./data/Rank1_Xs1.mat\")\n",
    "val_file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
